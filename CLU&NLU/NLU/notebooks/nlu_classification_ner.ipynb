{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7477776",
   "metadata": {},
   "source": [
    "# Natural Language Understanding: Text Classification & Named Entity Recognition\n",
    "\n",
    "This notebook demonstrates comprehensive NLU tasks using Hugging Face Transformers and spaCy for text classification and entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers spacy torch scikit-learn matplotlib seaborn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8350c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a05bf8",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Text classification is the task of assigning predefined categories or labels to text documents. We'll explore different types of classification tasks including sentiment analysis, topic classification, and emotion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e0d14",
   "metadata": {},
   "source": [
    "### 1. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis determines the emotional tone of text (positive, negative, neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analysis pipeline\n",
    "sentiment_classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Test examples\n",
    "texts = [\n",
    "    \"The movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"This product is terrible and completely useless.\",\n",
    "    \"The weather is okay today, nothing special.\",\n",
    "    \"I'm so excited about the upcoming vacation!\",\n",
    "    \"The service at the restaurant was disappointing.\"\n",
    "]\n",
    "\n",
    "print(\"🎭 Sentiment Analysis Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(texts, 1):\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    emoji = \"😊\" if result['label'] == 'POSITIVE' else \"😔\"\n",
    "    \n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   {emoji} Sentiment: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f641ab",
   "metadata": {},
   "source": [
    "### 2. Emotion Detection\n",
    "\n",
    "Emotion detection goes beyond sentiment to identify specific emotions like joy, anger, fear, sadness, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e363e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize emotion detection pipeline\n",
    "emotion_classifier = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base')\n",
    "\n",
    "# Test examples for emotion detection\n",
    "emotion_texts = [\n",
    "    \"I can't believe I won the lottery! This is amazing!\",\n",
    "    \"I'm so frustrated with this constant technical issues.\",\n",
    "    \"The thunderstorm outside is making me feel anxious.\",\n",
    "    \"I feel so lonely since moving to this new city.\",\n",
    "    \"That horror movie really scared me last night.\",\n",
    "    \"I'm disgusted by the way they treated the animals.\"\n",
    "]\n",
    "\n",
    "print(\"😊 Emotion Detection Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "emotion_emoji_map = {\n",
    "    'joy': '😄', 'anger': '😠', 'fear': '😨', \n",
    "    'sadness': '😢', 'surprise': '😲', 'disgust': '🤢'\n",
    "}\n",
    "\n",
    "for i, text in enumerate(emotion_texts, 1):\n",
    "    result = emotion_classifier(text)[0]\n",
    "    emotion = result['label'].lower()\n",
    "    emoji = emotion_emoji_map.get(emotion, '😐')\n",
    "    \n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   {emoji} Emotion: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b61d17",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition identifies and classifies named entities in text into predefined categories such as persons, organizations, locations, dates, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade9d0e",
   "metadata": {},
   "source": [
    "### 1. spaCy NER\n",
    "\n",
    "spaCy provides robust named entity recognition with pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Test examples for NER\n",
    "ner_texts = [\n",
    "    \"Apple Inc. is looking at buying U.K. startup for $1 billion on December 15, 2023.\",\n",
    "    \"Elon Musk founded SpaceX in 2002 and later became CEO of Tesla in Palo Alto, California.\",\n",
    "    \"The meeting with Microsoft will be held at 3 PM EST in New York City next Tuesday.\",\n",
    "    \"Dr. Jane Smith from Harvard University published a research paper in Nature journal.\",\n",
    "    \"Amazon announced a new AWS data center in Frankfurt, Germany, investing €2.8 billion.\"\n",
    "]\n",
    "\n",
    "print(\"🏷️  spaCy Named Entity Recognition\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Entity type descriptions\n",
    "entity_descriptions = {\n",
    "    'PERSON': '👤 Person names',\n",
    "    'ORG': '🏢 Organizations, companies',\n",
    "    'GPE': '🌍 Countries, cities, states',\n",
    "    'MONEY': '💰 Monetary values',\n",
    "    'DATE': '📅 Dates and times',\n",
    "    'TIME': '⏰ Times',\n",
    "    'PRODUCT': '📦 Products, vehicles',\n",
    "    'EVENT': '🎉 Events',\n",
    "    'FAC': '🏛️  Buildings, facilities',\n",
    "    'LOC': '📍 Locations',\n",
    "    'NORP': '🏛️  Nationalities, religious groups'\n",
    "}\n",
    "\n",
    "for i, text in enumerate(ner_texts, 1):\n",
    "    doc = nlp(text)\n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(\"   Entities found:\")\n",
    "    \n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            desc = entity_descriptions.get(ent.label_, f\"📌 {ent.label_}\")\n",
    "            print(f\"      • {ent.text} → {desc}\")\n",
    "    else:\n",
    "        print(\"      • No entities found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc758140",
   "metadata": {},
   "source": [
    "### 2. Transformers NER\n",
    "\n",
    "Hugging Face Transformers also provides powerful NER models with different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdd2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Transformers NER pipeline\n",
    "ner_pipeline = pipeline('ner', model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True)\n",
    "\n",
    "# Test text for comparison\n",
    "test_text = \"Apple Inc. is looking at buying U.K. startup for $1 billion on December 15, 2023.\"\n",
    "\n",
    "print(\"🤖 Transformers NER Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(\"\\nEntities found:\")\n",
    "\n",
    "entities = ner_pipeline(test_text)\n",
    "for entity in entities:\n",
    "    confidence_bar = \"█\" * int(entity['score'] * 10)\n",
    "    print(f\"  • {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.4f}) {confidence_bar}\")\n",
    "\n",
    "# Compare with spaCy results\n",
    "print(\"\\n🔄 Comparison with spaCy:\")\n",
    "print(\"=\" * 30)\n",
    "doc = nlp(test_text)\n",
    "print(\"spaCy entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"  • {ent.text}: {ent.label_}\")\n",
    "\n",
    "print(\"\\nTransformers entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"  • {entity['word']}: {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1f601",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's visualize the results of our NLU analysis to better understand the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment distribution\n",
    "sentiment_results = []\n",
    "for text in texts:\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    sentiment_results.append(result['label'])\n",
    "\n",
    "# Count sentiment distribution\n",
    "sentiment_counts = Counter(sentiment_results)\n",
    "\n",
    "# Create visualizations with robust error handling\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Sentiment Distribution with error handling\n",
    "if sentiment_results and sentiment_counts and len(sentiment_counts) > 0:\n",
    "    ax1.pie(sentiment_counts.values(), labels=sentiment_counts.keys(), autopct='%1.1f%%', \n",
    "            colors=['lightgreen', 'lightcoral'])\n",
    "    ax1.set_title('📊 Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No sentiment data available', \n",
    "             transform=ax1.transAxes, ha='center', va='center',\n",
    "             fontsize=12, style='italic')\n",
    "    ax1.set_title('📊 Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Emotion Distribution with error handling\n",
    "emotion_results = []\n",
    "for text in emotion_texts:\n",
    "    result = emotion_classifier(text)[0]\n",
    "    emotion_results.append(result['label'])\n",
    "\n",
    "emotion_counts = Counter(emotion_results)\n",
    "if emotion_results and emotion_counts and len(emotion_counts) > 0:\n",
    "    ax2.bar(emotion_counts.keys(), emotion_counts.values(), color='skyblue')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.set_title('😊 Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No emotion data available', \n",
    "             transform=ax2.transAxes, ha='center', va='center',\n",
    "             fontsize=12, style='italic')\n",
    "    ax2.set_title('😊 Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Entity Type Distribution (spaCy) with error handling\n",
    "all_entities = []\n",
    "for text in ner_texts:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        all_entities.append(ent.label_)\n",
    "\n",
    "entity_counts = Counter(all_entities)\n",
    "if all_entities and entity_counts and len(entity_counts) > 0:\n",
    "    ax3.barh(list(entity_counts.keys()), list(entity_counts.values()), color='lightgreen')\n",
    "    ax3.set_title('🏷️  Entity Types (spaCy)', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No entities detected', \n",
    "             transform=ax3.transAxes, ha='center', va='center',\n",
    "             fontsize=12, style='italic')\n",
    "    ax3.set_title('🏷️  Entity Types (spaCy)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Confidence Scores Distribution with error handling\n",
    "confidence_scores = []\n",
    "try:\n",
    "    for text in ner_texts:\n",
    "        entities = ner_pipeline(text)\n",
    "        for entity in entities:\n",
    "            if 'score' in entity and entity['score'] is not None:\n",
    "                confidence_scores.append(entity['score'])\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error processing confidence scores: {e}\")\n",
    "\n",
    "if confidence_scores and len(confidence_scores) > 0:\n",
    "    ax4.hist(confidence_scores, bins=min(10, len(confidence_scores)), \n",
    "             color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xlabel('Confidence Score')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('📈 NER Confidence Scores', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No confidence scores available', \n",
    "             transform=ax4.transAxes, ha='center', va='center',\n",
    "             fontsize=12, style='italic')\n",
    "    ax4.set_title('📈 NER Confidence Scores', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics with comprehensive error handling\n",
    "print(\"📈 Summary Statistics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Safe total count calculation\n",
    "total_texts = 0\n",
    "try:\n",
    "    total_texts = len(texts) + len(emotion_texts) + len(ner_texts)\n",
    "except (NameError, TypeError):\n",
    "    total_texts = 0\n",
    "\n",
    "print(f\"• Total texts analyzed: {total_texts}\")\n",
    "\n",
    "# Handle empty entity counts with safe max() operation\n",
    "if entity_counts and len(entity_counts) > 0:\n",
    "    try:\n",
    "        most_common_entity = max(entity_counts, key=entity_counts.get)\n",
    "        print(f\"• Unique entity types found: {len(entity_counts)}\")\n",
    "        print(f\"• Most common entity type: {most_common_entity}\")\n",
    "    except (ValueError, KeyError) as e:\n",
    "        print(f\"• Unique entity types found: {len(entity_counts) if entity_counts else 0}\")\n",
    "        print(\"• Most common entity type: Unable to determine\")\n",
    "else:\n",
    "    print(\"• Unique entity types found: 0\")\n",
    "    print(\"• Most common entity type: None\")\n",
    "\n",
    "# Handle empty confidence scores with division by zero protection\n",
    "if confidence_scores and len(confidence_scores) > 0:\n",
    "    try:\n",
    "        avg_confidence = sum(confidence_scores) / len(confidence_scores)\n",
    "        print(f\"• Average NER confidence: {avg_confidence:.4f}\")\n",
    "        print(f\"• Confidence score range: {min(confidence_scores):.4f} - {max(confidence_scores):.4f}\")\n",
    "    except (ZeroDivisionError, ValueError) as e:\n",
    "        print(\"• Average NER confidence: Unable to calculate\")\n",
    "else:\n",
    "    print(\"• Average NER confidence: N/A (no confidence scores available)\")\n",
    "\n",
    "# Handle empty sentiment and emotion counts with safe max() operation\n",
    "if sentiment_counts and len(sentiment_counts) > 0:\n",
    "    try:\n",
    "        most_common_sentiment = max(sentiment_counts, key=sentiment_counts.get)\n",
    "        print(f\"• Most common sentiment: {most_common_sentiment}\")\n",
    "    except (ValueError, KeyError):\n",
    "        print(\"• Most common sentiment: Unable to determine\")\n",
    "else:\n",
    "    print(\"• Most common sentiment: N/A\")\n",
    "\n",
    "if emotion_counts and len(emotion_counts) > 0:\n",
    "    try:\n",
    "        most_common_emotion = max(emotion_counts, key=emotion_counts.get)\n",
    "        print(f\"• Most common emotion: {most_common_emotion}\")\n",
    "    except (ValueError, KeyError):\n",
    "        print(\"• Most common emotion: Unable to determine\")\n",
    "else:\n",
    "    print(\"• Most common emotion: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a22cc",
   "metadata": {},
   "source": [
    "## Comprehensive NLU Analysis\n",
    "\n",
    "Let's create a unified function that performs all NLU tasks on any given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a841d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_nlu_analysis(text):\n",
    "    \"\"\"\n",
    "    Perform comprehensive NLU analysis including sentiment, emotion, and entity recognition\n",
    "    with robust error handling for all components\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        print(\"⚠️  Warning: Empty or invalid text provided\")\n",
    "        return None\n",
    "    \n",
    "    print(\"🔍 Comprehensive NLU Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📝 Input Text: {text}\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Sentiment Analysis with error handling\n",
    "    try:\n",
    "        sentiment_result = sentiment_classifier(text)[0]\n",
    "        sentiment_emoji = \"😊\" if sentiment_result['label'] == 'POSITIVE' else \"😔\"\n",
    "        print(f\"🎭 Sentiment: {sentiment_emoji} {sentiment_result['label']} (confidence: {sentiment_result['score']:.4f})\")\n",
    "        results['sentiment'] = sentiment_result\n",
    "    except Exception as e:\n",
    "        print(f\"🎭 Sentiment: ❌ Error during sentiment analysis: {e}\")\n",
    "        results['sentiment'] = None\n",
    "    \n",
    "    # Emotion Detection with error handling\n",
    "    try:\n",
    "        emotion_result = emotion_classifier(text)[0]\n",
    "        emotion = emotion_result['label'].lower()\n",
    "        emotion_emoji = emotion_emoji_map.get(emotion, '😐')\n",
    "        print(f\"😊 Emotion: {emotion_emoji} {emotion_result['label']} (confidence: {emotion_result['score']:.4f})\")\n",
    "        results['emotion'] = emotion_result\n",
    "    except Exception as e:\n",
    "        print(f\"😊 Emotion: ❌ Error during emotion detection: {e}\")\n",
    "        results['emotion'] = None\n",
    "    \n",
    "    # spaCy NER with error handling\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        print(\"🏷️  Entities (spaCy):\")\n",
    "        if doc.ents and len(doc.ents) > 0:\n",
    "            spacy_entities = []\n",
    "            for ent in doc.ents:\n",
    "                desc = entity_descriptions.get(ent.label_, f\"📌 {ent.label_}\")\n",
    "                print(f\"   • {ent.text} → {desc}\")\n",
    "                spacy_entities.append((ent.text, ent.label_))\n",
    "            results['spacy_entities'] = spacy_entities\n",
    "        else:\n",
    "            print(\"   • No entities found\")\n",
    "            results['spacy_entities'] = []\n",
    "    except Exception as e:\n",
    "        print(f\"🏷️  Entities (spaCy): ❌ Error during spaCy NER: {e}\")\n",
    "        results['spacy_entities'] = []\n",
    "    \n",
    "    # Transformers NER with error handling\n",
    "    try:\n",
    "        entities = ner_pipeline(text)\n",
    "        print(\"🤖 Entities (Transformers):\")\n",
    "        if entities and len(entities) > 0:\n",
    "            transformer_entities = []\n",
    "            for entity in entities:\n",
    "                if all(key in entity for key in ['word', 'entity_group', 'score']):\n",
    "                    print(f\"   • {entity['word']} → {entity['entity_group']} (confidence: {entity['score']:.4f})\")\n",
    "                    transformer_entities.append((entity['word'], entity['entity_group'], entity['score']))\n",
    "                else:\n",
    "                    print(f\"   • {entity.get('word', 'Unknown')} → {entity.get('entity_group', 'Unknown')} (incomplete data)\")\n",
    "            results['transformer_entities'] = transformer_entities\n",
    "        else:\n",
    "            print(\"   • No entities found\")\n",
    "            results['transformer_entities'] = []\n",
    "    except Exception as e:\n",
    "        print(f\"🤖 Entities (Transformers): ❌ Error during Transformers NER: {e}\")\n",
    "        results['transformer_entities'] = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the comprehensive function with enhanced error handling\n",
    "test_samples = [\n",
    "    \"I'm thrilled to announce that our company Apple Inc. will be launching a new product in San Francisco next month!\",\n",
    "    \"The disappointing performance by Tesla in the stock market yesterday really worried investors in New York.\",\n",
    "    \"Dr. Sarah Johnson from MIT published groundbreaking research about artificial intelligence in Nature journal last week.\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Comprehensive Analysis Function\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\n📝 Test Sample {i}:\")\n",
    "    result = comprehensive_nlu_analysis(sample)\n",
    "    \n",
    "    # Validate results\n",
    "    if result is not None:\n",
    "        print(\"✅ Analysis completed successfully\")\n",
    "        \n",
    "        # Count successful components\n",
    "        successful_components = sum(1 for key, value in result.items() if value is not None and value != [])\n",
    "        total_components = len(result)\n",
    "        print(f\"📊 Components processed: {successful_components}/{total_components}\")\n",
    "    else:\n",
    "        print(\"❌ Analysis failed\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f61f03",
   "metadata": {},
   "source": [
    "## Interactive Experimentation\n",
    "\n",
    "Try your own text examples below! Modify the `user_text` variable to experiment with different NLU scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Experiment with your own text!\n",
    "# Modify this variable to test different texts\n",
    "user_text = \"I absolutely love the new iPhone released by Apple in Cupertino! Tim Cook did an amazing job presenting it.\"\n",
    "\n",
    "# Validate input text\n",
    "if not user_text or not isinstance(user_text, str) or len(user_text.strip()) == 0:\n",
    "    print(\"⚠️  Warning: Please provide a valid text string for analysis\")\n",
    "    user_text = \"Default sample text for analysis.\"\n",
    "\n",
    "print(\"🧪 Your Custom Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run comprehensive analysis with error handling\n",
    "try:\n",
    "    result = comprehensive_nlu_analysis(user_text)\n",
    "    \n",
    "    if result is not None:\n",
    "        # Additional analysis: Word count and text statistics with error handling\n",
    "        print(\"📊 Text Statistics:\")\n",
    "        \n",
    "        try:\n",
    "            words = user_text.split()\n",
    "            word_count = len(words) if words else 0\n",
    "            char_count = len(user_text) if user_text else 0\n",
    "            \n",
    "            # Safe sentence counting\n",
    "            sentences = [s for s in user_text.split('.') if s.strip()] if user_text else []\n",
    "            sentence_count = len(sentences)\n",
    "            \n",
    "            print(f\"   • Word count: {word_count}\")\n",
    "            print(f\"   • Character count: {char_count}\")\n",
    "            print(f\"   • Sentence count: {sentence_count}\")\n",
    "            \n",
    "            # Entity density calculations with error handling\n",
    "            spacy_entity_count = len(result.get('spacy_entities', []))\n",
    "            transformer_entity_count = len(result.get('transformer_entities', []))\n",
    "            \n",
    "            if word_count > 0:\n",
    "                spacy_density = (spacy_entity_count / word_count) * 100\n",
    "                transformer_density = (transformer_entity_count / word_count) * 100\n",
    "                print(f\"   • Entity density (spaCy): {spacy_density:.2f}%\")\n",
    "                print(f\"   • Entity density (Transformers): {transformer_density:.2f}%\")\n",
    "            else:\n",
    "                print(\"   • Entity density: Cannot calculate (no words found)\")\n",
    "                \n",
    "            # Additional insights\n",
    "            if spacy_entity_count > 0 or transformer_entity_count > 0:\n",
    "                print(f\"   • Total unique entities detected: {spacy_entity_count + transformer_entity_count}\")\n",
    "            else:\n",
    "                print(\"   • Total unique entities detected: 0\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating text statistics: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ Analysis failed. Please check your input text.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error during analysis: {e}\")\n",
    "\n",
    "print(\"\\n💡 Try modifying the 'user_text' variable above with your own examples!\")\n",
    "print(\"💡 Tips for better analysis:\")\n",
    "print(\"   • Use text with names, places, or organizations for entity recognition\")\n",
    "print(\"   • Include emotional words for better sentiment/emotion detection\")\n",
    "print(\"   • Try different text lengths and styles\")\n",
    "print(\"   • Test with multilingual content to see model limitations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advanced Visualization Utilities\n",
    "\n",
    "def create_safe_visualization(data, plot_type, title, **kwargs):\n",
    "    \"\"\"\n",
    "    Create visualizations with comprehensive error handling\n",
    "    \n",
    "    Args:\n",
    "        data: Data to visualize (dict, list, etc.)\n",
    "        plot_type: Type of plot ('pie', 'bar', 'hist', 'barh')\n",
    "        title: Plot title\n",
    "        **kwargs: Additional plot parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=kwargs.get('figsize', (8, 6)))\n",
    "        \n",
    "        # Check if data is empty or invalid\n",
    "        if not data or (isinstance(data, (list, dict)) and len(data) == 0):\n",
    "            ax.text(0.5, 0.5, 'No data available for visualization', \n",
    "                   transform=ax.transAxes, ha='center', va='center',\n",
    "                   fontsize=12, style='italic')\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            return fig, ax\n",
    "        \n",
    "        # Handle different plot types\n",
    "        if plot_type == 'pie' and isinstance(data, dict):\n",
    "            if sum(data.values()) > 0:  # Ensure we have non-zero values\n",
    "                ax.pie(data.values(), labels=data.keys(), autopct='%1.1f%%',\n",
    "                      colors=kwargs.get('colors', None))\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'All values are zero', \n",
    "                       transform=ax.transAxes, ha='center', va='center',\n",
    "                       fontsize=12, style='italic')\n",
    "                \n",
    "        elif plot_type == 'bar' and isinstance(data, dict):\n",
    "            ax.bar(data.keys(), data.values(), \n",
    "                  color=kwargs.get('color', 'skyblue'))\n",
    "            if kwargs.get('rotate_labels', False):\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "                \n",
    "        elif plot_type == 'barh' and isinstance(data, dict):\n",
    "            ax.barh(list(data.keys()), list(data.values()),\n",
    "                   color=kwargs.get('color', 'lightgreen'))\n",
    "                   \n",
    "        elif plot_type == 'hist' and isinstance(data, list):\n",
    "            if len(data) > 0:\n",
    "                bins = min(kwargs.get('bins', 10), len(data))\n",
    "                ax.hist(data, bins=bins, \n",
    "                       color=kwargs.get('color', 'orange'),\n",
    "                       alpha=kwargs.get('alpha', 0.7),\n",
    "                       edgecolor=kwargs.get('edgecolor', 'black'))\n",
    "                ax.set_xlabel(kwargs.get('xlabel', 'Values'))\n",
    "                ax.set_ylabel(kwargs.get('ylabel', 'Frequency'))\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'No data points to plot', \n",
    "                       transform=ax.transAxes, ha='center', va='center',\n",
    "                       fontsize=12, style='italic')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'Unsupported plot type: {plot_type}', \n",
    "                   transform=ax.transAxes, ha='center', va='center',\n",
    "                   fontsize=12, style='italic')\n",
    "        \n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        return fig, ax\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Create error plot\n",
    "        fig, ax = plt.subplots(figsize=kwargs.get('figsize', (8, 6)))\n",
    "        ax.text(0.5, 0.5, f'Error creating visualization:\\n{str(e)}', \n",
    "               transform=ax.transAxes, ha='center', va='center',\n",
    "               fontsize=12, style='italic', color='red')\n",
    "        ax.set_title(f'{title} (Error)', fontsize=14, fontweight='bold')\n",
    "        return fig, ax\n",
    "\n",
    "def analyze_text_batch(texts, max_texts=None):\n",
    "    \"\"\"\n",
    "    Analyze a batch of texts with progress tracking and error handling\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to analyze\n",
    "        max_texts: Maximum number of texts to process (for large batches)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not texts or not isinstance(texts, list):\n",
    "        print(\"❌ Error: Please provide a valid list of texts\")\n",
    "        return None\n",
    "    \n",
    "    # Limit processing for performance\n",
    "    if max_texts and len(texts) > max_texts:\n",
    "        print(f\"📊 Processing first {max_texts} texts out of {len(texts)} total\")\n",
    "        texts = texts[:max_texts]\n",
    "    \n",
    "    results = {\n",
    "        'sentiments': [],\n",
    "        'emotions': [],\n",
    "        'entities': [],\n",
    "        'confidence_scores': [],\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    print(f\"🔍 Analyzing {len(texts)} texts...\")\n",
    "    \n",
    "    for i, text in enumerate(texts, 1):\n",
    "        try:\n",
    "            # Progress indicator\n",
    "            if i % 5 == 0 or i == len(texts):\n",
    "                progress = (i / len(texts)) * 100\n",
    "                print(f\"   Progress: {progress:.1f}% ({i}/{len(texts)})\")\n",
    "            \n",
    "            result = comprehensive_nlu_analysis(text)\n",
    "            \n",
    "            if result:\n",
    "                # Collect sentiment data\n",
    "                if result.get('sentiment'):\n",
    "                    results['sentiments'].append(result['sentiment']['label'])\n",
    "                \n",
    "                # Collect emotion data\n",
    "                if result.get('emotion'):\n",
    "                    results['emotions'].append(result['emotion']['label'])\n",
    "                \n",
    "                # Collect entity data\n",
    "                if result.get('spacy_entities'):\n",
    "                    results['entities'].extend([ent[1] for ent in result['spacy_entities']])\n",
    "                \n",
    "                # Collect confidence scores\n",
    "                if result.get('transformer_entities'):\n",
    "                    results['confidence_scores'].extend([ent[2] for ent in result['transformer_entities']])\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['errors'].append(f\"Text {i}: {str(e)}\")\n",
    "    \n",
    "    print(f\"✅ Analysis complete! Processed {len(texts)} texts with {len(results['errors'])} errors\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Batch analysis with visualization\n",
    "sample_texts = [\n",
    "    \"I love this new technology!\",\n",
    "    \"This is frustrating and disappointing.\",\n",
    "    \"Apple Inc. announced a new product.\",\n",
    "    \"I'm excited about the upcoming conference in New York.\",\n",
    "    \"The weather is neutral today.\"\n",
    "]\n",
    "\n",
    "print(\"🚀 Demonstrating Batch Analysis with Safe Visualizations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    batch_results = analyze_text_batch(sample_texts)\n",
    "    \n",
    "    if batch_results:\n",
    "        # Create visualizations with error handling\n",
    "        sentiment_counts = Counter(batch_results['sentiments'])\n",
    "        emotion_counts = Counter(batch_results['emotions'])\n",
    "        entity_counts = Counter(batch_results['entities'])\n",
    "        \n",
    "        # Create multiple plots\n",
    "        fig1, ax1 = create_safe_visualization(\n",
    "            sentiment_counts, 'pie', '📊 Batch Sentiment Analysis',\n",
    "            colors=['lightgreen', 'lightcoral'], figsize=(8, 6)\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        fig2, ax2 = create_safe_visualization(\n",
    "            emotion_counts, 'bar', '😊 Batch Emotion Analysis',\n",
    "            color='skyblue', rotate_labels=True, figsize=(10, 6)\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        fig3, ax3 = create_safe_visualization(\n",
    "            entity_counts, 'barh', '🏷️ Batch Entity Analysis',\n",
    "            color='lightgreen', figsize=(10, 6)\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        fig4, ax4 = create_safe_visualization(\n",
    "            batch_results['confidence_scores'], 'hist', '📈 Confidence Score Distribution',\n",
    "            color='orange', alpha=0.7, bins=10, \n",
    "            xlabel='Confidence Score', ylabel='Frequency', figsize=(8, 6)\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary with error handling\n",
    "        print(\"\\n📈 Batch Analysis Summary\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        safe_counts = {\n",
    "            'sentiments': len(batch_results['sentiments']),\n",
    "            'emotions': len(batch_results['emotions']),\n",
    "            'entities': len(batch_results['entities']),\n",
    "            'confidence_scores': len(batch_results['confidence_scores'])\n",
    "        }\n",
    "        \n",
    "        for category, count in safe_counts.items():\n",
    "            print(f\"• {category.title()}: {count} items\")\n",
    "        \n",
    "        if batch_results['errors']:\n",
    "            print(f\"• Errors encountered: {len(batch_results['errors'])}\")\n",
    "            for error in batch_results['errors'][:3]:  # Show first 3 errors\n",
    "                print(f\"  - {error}\")\n",
    "    else:\n",
    "        print(\"❌ Batch analysis failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during batch analysis: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
