{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7477776",
   "metadata": {},
   "source": [
    "# Natural Language Understanding: Text Classification & Named Entity Recognition\n",
    "\n",
    "This notebook demonstrates comprehensive NLU tasks using Hugging Face Transformers and spaCy for text classification and entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers spacy torch scikit-learn matplotlib seaborn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8350c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a05bf8",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Text classification is the task of assigning predefined categories or labels to text documents. We'll explore different types of classification tasks including sentiment analysis, topic classification, and emotion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e0d14",
   "metadata": {},
   "source": [
    "### 1. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis determines the emotional tone of text (positive, negative, neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analysis pipeline\n",
    "sentiment_classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Test examples\n",
    "texts = [\n",
    "    \"The movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"This product is terrible and completely useless.\",\n",
    "    \"The weather is okay today, nothing special.\",\n",
    "    \"I'm so excited about the upcoming vacation!\",\n",
    "    \"The service at the restaurant was disappointing.\"\n",
    "]\n",
    "\n",
    "print(\"üé≠ Sentiment Analysis Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(texts, 1):\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    emoji = \"üòä\" if result['label'] == 'POSITIVE' else \"üòî\"\n",
    "    \n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   {emoji} Sentiment: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f641ab",
   "metadata": {},
   "source": [
    "### 2. Emotion Detection\n",
    "\n",
    "Emotion detection goes beyond sentiment to identify specific emotions like joy, anger, fear, sadness, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e363e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize emotion detection pipeline\n",
    "emotion_classifier = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base')\n",
    "\n",
    "# Test examples for emotion detection\n",
    "emotion_texts = [\n",
    "    \"I can't believe I won the lottery! This is amazing!\",\n",
    "    \"I'm so frustrated with this constant technical issues.\",\n",
    "    \"The thunderstorm outside is making me feel anxious.\",\n",
    "    \"I feel so lonely since moving to this new city.\",\n",
    "    \"That horror movie really scared me last night.\",\n",
    "    \"I'm disgusted by the way they treated the animals.\"\n",
    "]\n",
    "\n",
    "print(\"üòä Emotion Detection Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "emotion_emoji_map = {\n",
    "    'joy': 'üòÑ', 'anger': 'üò†', 'fear': 'üò®', \n",
    "    'sadness': 'üò¢', 'surprise': 'üò≤', 'disgust': 'ü§¢'\n",
    "}\n",
    "\n",
    "for i, text in enumerate(emotion_texts, 1):\n",
    "    result = emotion_classifier(text)[0]\n",
    "    emotion = result['label'].lower()\n",
    "    emoji = emotion_emoji_map.get(emotion, 'üòê')\n",
    "    \n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   {emoji} Emotion: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b61d17",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition identifies and classifies named entities in text into predefined categories such as persons, organizations, locations, dates, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade9d0e",
   "metadata": {},
   "source": [
    "### 1. spaCy NER\n",
    "\n",
    "spaCy provides robust named entity recognition with pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Test examples for NER\n",
    "ner_texts = [\n",
    "    \"Apple Inc. is looking at buying U.K. startup for $1 billion on December 15, 2023.\",\n",
    "    \"Elon Musk founded SpaceX in 2002 and later became CEO of Tesla in Palo Alto, California.\",\n",
    "    \"The meeting with Microsoft will be held at 3 PM EST in New York City next Tuesday.\",\n",
    "    \"Dr. Jane Smith from Harvard University published a research paper in Nature journal.\",\n",
    "    \"Amazon announced a new AWS data center in Frankfurt, Germany, investing ‚Ç¨2.8 billion.\"\n",
    "]\n",
    "\n",
    "print(\"üè∑Ô∏è  spaCy Named Entity Recognition\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Entity type descriptions\n",
    "entity_descriptions = {\n",
    "    'PERSON': 'üë§ Person names',\n",
    "    'ORG': 'üè¢ Organizations, companies',\n",
    "    'GPE': 'üåç Countries, cities, states',\n",
    "    'MONEY': 'üí∞ Monetary values',\n",
    "    'DATE': 'üìÖ Dates and times',\n",
    "    'TIME': '‚è∞ Times',\n",
    "    'PRODUCT': 'üì¶ Products, vehicles',\n",
    "    'EVENT': 'üéâ Events',\n",
    "    'FAC': 'üèõÔ∏è  Buildings, facilities',\n",
    "    'LOC': 'üìç Locations',\n",
    "    'NORP': 'üèõÔ∏è  Nationalities, religious groups'\n",
    "}\n",
    "\n",
    "for i, text in enumerate(ner_texts, 1):\n",
    "    doc = nlp(text)\n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(\"   Entities found:\")\n",
    "    \n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            desc = entity_descriptions.get(ent.label_, f\"üìå {ent.label_}\")\n",
    "            print(f\"      ‚Ä¢ {ent.text} ‚Üí {desc}\")\n",
    "    else:\n",
    "        print(\"      ‚Ä¢ No entities found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc758140",
   "metadata": {},
   "source": [
    "### 2. Transformers NER\n",
    "\n",
    "Hugging Face Transformers also provides powerful NER models with different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdd2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Transformers NER pipeline\n",
    "ner_pipeline = pipeline('ner', model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True)\n",
    "\n",
    "# Test text for comparison\n",
    "test_text = \"Apple Inc. is looking at buying U.K. startup for $1 billion on December 15, 2023.\"\n",
    "\n",
    "print(\"ü§ñ Transformers NER Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(\"\\nEntities found:\")\n",
    "\n",
    "entities = ner_pipeline(test_text)\n",
    "for entity in entities:\n",
    "    confidence_bar = \"‚ñà\" * int(entity['score'] * 10)\n",
    "    print(f\"  ‚Ä¢ {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.4f}) {confidence_bar}\")\n",
    "\n",
    "# Compare with spaCy results\n",
    "print(\"\\nüîÑ Comparison with spaCy:\")\n",
    "print(\"=\" * 30)\n",
    "doc = nlp(test_text)\n",
    "print(\"spaCy entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"  ‚Ä¢ {ent.text}: {ent.label_}\")\n",
    "\n",
    "print(\"\\nTransformers entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"  ‚Ä¢ {entity['word']}: {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1f601",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's visualize the results of our NLU analysis to better understand the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment distribution\n",
    "sentiment_results = []\n",
    "for text in texts:\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    sentiment_results.append(result['label'])\n",
    "\n",
    "# Count sentiment distribution\n",
    "sentiment_counts = Counter(sentiment_results)\n",
    "\n",
    "# Create visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Sentiment Distribution\n",
    "ax1.pie(sentiment_counts.values(), labels=sentiment_counts.keys(), autopct='%1.1f%%', \n",
    "        colors=['lightgreen', 'lightcoral'])\n",
    "ax1.set_title('üìä Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Emotion Distribution\n",
    "emotion_results = []\n",
    "for text in emotion_texts:\n",
    "    result = emotion_classifier(text)[0]\n",
    "    emotion_results.append(result['label'])\n",
    "\n",
    "emotion_counts = Counter(emotion_results)\n",
    "ax2.bar(emotion_counts.keys(), emotion_counts.values(), color='skyblue')\n",
    "ax2.set_title('üòä Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Entity Type Distribution (spaCy)\n",
    "all_entities = []\n",
    "for text in ner_texts:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        all_entities.append(ent.label_)\n",
    "\n",
    "entity_counts = Counter(all_entities)\n",
    "ax3.barh(list(entity_counts.keys()), list(entity_counts.values()), color='lightgreen')\n",
    "ax3.set_title('üè∑Ô∏è  Entity Types (spaCy)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Confidence Scores Distribution\n",
    "confidence_scores = []\n",
    "for text in ner_texts:\n",
    "    entities = ner_pipeline(text)\n",
    "    for entity in entities:\n",
    "        confidence_scores.append(entity['score'])\n",
    "\n",
    "ax4.hist(confidence_scores, bins=10, color='orange', alpha=0.7, edgecolor='black')\n",
    "ax4.set_title('üìà NER Confidence Scores', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Confidence Score')\n",
    "ax4.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"üìà Summary Statistics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"‚Ä¢ Total texts analyzed: {len(texts + emotion_texts + ner_texts)}\")\n",
    "print(f\"‚Ä¢ Unique entity types found: {len(entity_counts)}\")\n",
    "print(f\"‚Ä¢ Average NER confidence: {sum(confidence_scores)/len(confidence_scores):.4f}\")\n",
    "print(f\"‚Ä¢ Most common sentiment: {max(sentiment_counts, key=sentiment_counts.get)}\")\n",
    "print(f\"‚Ä¢ Most common emotion: {max(emotion_counts, key=emotion_counts.get)}\")\n",
    "print(f\"‚Ä¢ Most common entity type: {max(entity_counts, key=entity_counts.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a22cc",
   "metadata": {},
   "source": [
    "## Comprehensive NLU Analysis\n",
    "\n",
    "Let's create a unified function that performs all NLU tasks on any given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a841d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_nlu_analysis(text):\n",
    "    \"\"\"\n",
    "    Perform comprehensive NLU analysis including sentiment, emotion, and entity recognition\n",
    "    \"\"\"\n",
    "    print(\"üîç Comprehensive NLU Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìù Input Text: {text}\")\n",
    "    print()\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    sentiment_result = sentiment_classifier(text)[0]\n",
    "    sentiment_emoji = \"üòä\" if sentiment_result['label'] == 'POSITIVE' else \"üòî\"\n",
    "    print(f\"üé≠ Sentiment: {sentiment_emoji} {sentiment_result['label']} (confidence: {sentiment_result['score']:.4f})\")\n",
    "    \n",
    "    # Emotion Detection\n",
    "    emotion_result = emotion_classifier(text)[0]\n",
    "    emotion = emotion_result['label'].lower()\n",
    "    emotion_emoji = emotion_emoji_map.get(emotion, 'üòê')\n",
    "    print(f\"üòä Emotion: {emotion_emoji} {emotion_result['label']} (confidence: {emotion_result['score']:.4f})\")\n",
    "    \n",
    "    # spaCy NER\n",
    "    doc = nlp(text)\n",
    "    print(\"üè∑Ô∏è  Entities (spaCy):\")\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            desc = entity_descriptions.get(ent.label_, f\"üìå {ent.label_}\")\n",
    "            print(f\"   ‚Ä¢ {ent.text} ‚Üí {desc}\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ No entities found\")\n",
    "    \n",
    "    # Transformers NER\n",
    "    entities = ner_pipeline(text)\n",
    "    print(\"ü§ñ Entities (Transformers):\")\n",
    "    if entities:\n",
    "        for entity in entities:\n",
    "            print(f\"   ‚Ä¢ {entity['word']} ‚Üí {entity['entity_group']} (confidence: {entity['score']:.4f})\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ No entities found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment_result,\n",
    "        'emotion': emotion_result,\n",
    "        'spacy_entities': [(ent.text, ent.label_) for ent in doc.ents],\n",
    "        'transformer_entities': [(e['word'], e['entity_group'], e['score']) for e in entities]\n",
    "    }\n",
    "\n",
    "# Test the comprehensive function\n",
    "test_samples = [\n",
    "    \"I'm thrilled to announce that our company Apple Inc. will be launching a new product in San Francisco next month!\",\n",
    "    \"The disappointing performance by Tesla in the stock market yesterday really worried investors in New York.\",\n",
    "    \"Dr. Sarah Johnson from MIT published groundbreaking research about artificial intelligence in Nature journal last week.\"\n",
    "]\n",
    "\n",
    "for sample in test_samples:\n",
    "    result = comprehensive_nlu_analysis(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f61f03",
   "metadata": {},
   "source": [
    "## Interactive Experimentation\n",
    "\n",
    "Try your own text examples below! Modify the `user_text` variable to experiment with different NLU scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Experiment with your own text!\n",
    "# Modify this variable to test different texts\n",
    "user_text = \"I absolutely love the new iPhone released by Apple in Cupertino! Tim Cook did an amazing job presenting it.\"\n",
    "\n",
    "# Run comprehensive analysis\n",
    "print(\"üß™ Your Custom Analysis\")\n",
    "result = comprehensive_nlu_analysis(user_text)\n",
    "\n",
    "# Additional analysis: Word count and text statistics\n",
    "words = user_text.split()\n",
    "print(\"üìä Text Statistics:\")\n",
    "print(f\"   ‚Ä¢ Word count: {len(words)}\")\n",
    "print(f\"   ‚Ä¢ Character count: {len(user_text)}\")\n",
    "print(f\"   ‚Ä¢ Sentence count: {len([s for s in user_text.split('.') if s.strip()])}\")\n",
    "\n",
    "# Entity density\n",
    "spacy_entity_count = len(result['spacy_entities'])\n",
    "transformer_entity_count = len(result['transformer_entities'])\n",
    "print(f\"   ‚Ä¢ Entity density (spaCy): {spacy_entity_count/len(words):.2%}\")\n",
    "print(f\"   ‚Ä¢ Entity density (Transformers): {transformer_entity_count/len(words):.2%}\")\n",
    "\n",
    "print(\"\\nüí° Try modifying the 'user_text' variable above with your own examples!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
