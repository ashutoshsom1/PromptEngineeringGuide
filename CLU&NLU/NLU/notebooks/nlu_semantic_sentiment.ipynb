{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f3b975",
   "metadata": {},
   "source": [
    "# Natural Language Understanding: Semantic Parsing & Advanced Sentiment Analysis\n",
    "\n",
    "This notebook explores advanced NLU techniques including semantic parsing, structured data extraction, and multi-dimensional sentiment analysis using state-of-the-art transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef38370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch pandas matplotlib seaborn plotly wordcloud\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac40c77",
   "metadata": {},
   "source": [
    "## Semantic Parsing\n",
    "\n",
    "Semantic parsing converts natural language into structured representations that machines can understand and execute. This includes generating SQL queries, API calls, logical forms, and structured data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfaa32",
   "metadata": {},
   "source": [
    "### 1. Text-to-SQL Generation\n",
    "\n",
    "Converting natural language queries into SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11088001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text-to-SQL pipeline\n",
    "sql_generator = pipeline('text2text-generation', model='t5-small')\n",
    "\n",
    "# Natural language to SQL examples\n",
    "nl_queries = [\n",
    "    \"Show me all customers from New York\",\n",
    "    \"Find products with price greater than 100 dollars\",\n",
    "    \"Get the total sales for each month in 2023\",\n",
    "    \"List employees who joined after January 2022\",\n",
    "    \"Count the number of orders per customer\"\n",
    "]\n",
    "\n",
    "print(\"üóÉÔ∏è  Text-to-SQL Generation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_sql(nl_query, context=\"Database with tables: customers, products, orders, employees\"):\n",
    "    \"\"\"Generate SQL from natural language query\"\"\"\n",
    "    prompt = f\"Translate to SQL: {nl_query}. Context: {context}\"\n",
    "    result = sql_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "for i, query in enumerate(nl_queries, 1):\n",
    "    sql_result = generate_sql(query)\n",
    "    print(f\"{i}. Natural Language: {query}\")\n",
    "    print(f\"   üíæ Generated SQL: {sql_result}\")\n",
    "    print()\n",
    "\n",
    "# Custom query example\n",
    "print(\"üîß Custom Query:\")\n",
    "custom_query = \"Show me the average salary of employees in the engineering department\"\n",
    "custom_sql = generate_sql(custom_query)\n",
    "print(f\"üìù Query: {custom_query}\")\n",
    "print(f\"üíæ SQL: {custom_sql}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d886acc",
   "metadata": {},
   "source": [
    "### 2. Structured Data Extraction\n",
    "\n",
    "Extracting structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f925822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize structured extraction pipeline\n",
    "structure_generator = pipeline('text2text-generation', model='t5-small')\n",
    "\n",
    "# Sample unstructured texts\n",
    "unstructured_texts = [\n",
    "    \"John Smith, age 35, lives at 123 Main Street, New York, NY. He works as a Software Engineer at Tech Corp and earns $120,000 annually.\",\n",
    "    \"The iPhone 14 Pro costs $999 and was released on September 16, 2022. It features a 6.1-inch display and comes in four colors: Space Black, Silver, Gold, and Deep Purple.\",\n",
    "    \"Flight AA123 departs from JFK Airport at 3:45 PM on July 15th, 2024, arriving at LAX at 7:20 PM. The flight duration is 5 hours and 35 minutes.\"\n",
    "]\n",
    "\n",
    "print(\"üìä Structured Data Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_structured_data(text, schema=\"JSON\"):\n",
    "    \"\"\"Extract structured data from unstructured text\"\"\"\n",
    "    prompt = f\"Extract structured data as {schema}: {text}\"\n",
    "    result = structure_generator(prompt, max_length=150, num_return_sequences=1)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "for i, text in enumerate(unstructured_texts, 1):\n",
    "    print(f\"{i}. Original Text:\")\n",
    "    print(f\"   üìù {text}\")\n",
    "    \n",
    "    # Extract as JSON\n",
    "    json_structure = extract_structured_data(text, \"JSON\")\n",
    "    print(f\"   üìä Structured JSON: {json_structure}\")\n",
    "    \n",
    "    # Extract as key-value pairs\n",
    "    kv_structure = extract_structured_data(text, \"key-value pairs\")\n",
    "    print(f\"   üîë Key-Value: {kv_structure}\")\n",
    "    print()\n",
    "\n",
    "# Custom extraction\n",
    "print(\"üéØ Custom Extraction Example:\")\n",
    "custom_text = \"The quarterly sales report shows that Region A sold 1,500 units generating $450,000 revenue, while Region B sold 2,200 units with $660,000 revenue during Q3 2023.\"\n",
    "custom_extraction = extract_structured_data(custom_text, \"table format\")\n",
    "print(f\"üìù Text: {custom_text}\")\n",
    "print(f\"üìã Table: {custom_extraction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6f3cb",
   "metadata": {},
   "source": [
    "## Advanced Sentiment Analysis\n",
    "\n",
    "Going beyond basic positive/negative sentiment to explore multi-dimensional sentiment analysis, aspect-based sentiment, and emotion classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f5a6f",
   "metadata": {},
   "source": [
    "### 1. Multi-Dimensional Sentiment Analysis\n",
    "\n",
    "Analyzing sentiment across multiple dimensions: polarity, subjectivity, and emotional intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple sentiment analysis models\n",
    "basic_sentiment = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "emotion_classifier = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base')\n",
    "intensity_classifier = pipeline('text-classification', model='SamLowe/roberta-base-go_emotions')\n",
    "\n",
    "# Sample texts with varying sentiment complexity\n",
    "complex_texts = [\n",
    "    \"The movie had amazing visuals, but the plot was completely disappointing.\",  # Mixed sentiment\n",
    "    \"I'm absolutely thrilled about the new job opportunity, though I'm nervous about the challenges.\",  # Mixed emotions\n",
    "    \"The restaurant service was terrible, but the food was surprisingly excellent.\",  # Contrasting aspects\n",
    "    \"This product is okay, nothing special but does what it's supposed to do.\",  # Neutral with nuance\n",
    "    \"I'm devastated that the concert was cancelled, but I understand the safety concerns.\"  # Complex emotional state\n",
    "]\n",
    "\n",
    "print(\"üé≠ Multi-Dimensional Sentiment Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def analyze_complex_sentiment(text):\n",
    "    \"\"\"Perform comprehensive sentiment analysis\"\"\"\n",
    "    # Basic sentiment\n",
    "    basic_result = basic_sentiment(text)[0]\n",
    "    \n",
    "    # Emotion detection\n",
    "    emotion_result = emotion_classifier(text)[0]\n",
    "    \n",
    "    # Intensity/emotion spectrum\n",
    "    intensity_result = intensity_classifier(text)[0]\n",
    "    \n",
    "    return {\n",
    "        'basic_sentiment': basic_result,\n",
    "        'emotion': emotion_result,\n",
    "        'intensity': intensity_result\n",
    "    }\n",
    "\n",
    "for i, text in enumerate(complex_texts, 1):\n",
    "    results = analyze_complex_sentiment(text)\n",
    "    \n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   üéØ Basic Sentiment: {results['basic_sentiment']['label']} (confidence: {results['basic_sentiment']['score']:.4f})\")\n",
    "    print(f\"   üòä Emotion: {results['emotion']['label']} (confidence: {results['emotion']['score']:.4f})\")\n",
    "    print(f\"   üå°Ô∏è  Intensity: {results['intensity']['label']} (confidence: {results['intensity']['score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Visualization of sentiment dimensions\n",
    "sentiments = []\n",
    "emotions = []\n",
    "intensities = []\n",
    "\n",
    "for text in complex_texts:\n",
    "    results = analyze_complex_sentiment(text)\n",
    "    sentiments.append(results['basic_sentiment']['label'])\n",
    "    emotions.append(results['emotion']['label'])\n",
    "    intensities.append(results['intensity']['label'])\n",
    "\n",
    "# Create a summary dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Text': [f\"Text {i+1}\" for i in range(len(complex_texts))],\n",
    "    'Sentiment': sentiments,\n",
    "    'Emotion': emotions,\n",
    "    'Intensity': intensities\n",
    "})\n",
    "\n",
    "print(\"üìä Sentiment Analysis Summary:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d1736",
   "metadata": {},
   "source": [
    "### 2. Aspect-Based Sentiment Analysis\n",
    "\n",
    "Analyzing sentiment toward specific aspects or features mentioned in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ac15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect-based sentiment analysis using custom prompting\n",
    "def extract_aspects_and_sentiment(text):\n",
    "    \"\"\"Extract aspects and their associated sentiments\"\"\"\n",
    "    # Use text generation to identify aspects\n",
    "    aspect_prompt = f\"Extract the main aspects or features mentioned in this review: {text}\"\n",
    "    aspects_result = structure_generator(aspect_prompt, max_length=50)\n",
    "    \n",
    "    # Analyze sentiment for each identified aspect\n",
    "    aspects_text = aspects_result[0]['generated_text']\n",
    "    aspects = [aspect.strip() for aspect in aspects_text.split(',') if aspect.strip()]\n",
    "    \n",
    "    aspect_sentiments = {}\n",
    "    for aspect in aspects:\n",
    "        # Analyze sentiment specifically for this aspect\n",
    "        sentiment_result = basic_sentiment(f\"Regarding {aspect}: {text}\")[0]\n",
    "        aspect_sentiments[aspect] = sentiment_result\n",
    "    \n",
    "    return aspect_sentiments\n",
    "\n",
    "# Sample reviews with multiple aspects\n",
    "reviews = [\n",
    "    \"The hotel room was spacious and clean, but the service was slow and the food was mediocre.\",\n",
    "    \"I love the camera quality on this phone, but the battery life is terrible and it's too expensive.\",\n",
    "    \"The movie had brilliant acting and cinematography, though the storyline was confusing and too long.\",\n",
    "    \"The restaurant has amazing atmosphere and delicious desserts, but the main courses were overpriced.\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Aspect-Based Sentiment Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_aspect_results = []\n",
    "\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    print(f\"{i}. Review: {review}\")\n",
    "    \n",
    "    aspect_sentiments = extract_aspects_and_sentiment(review)\n",
    "    all_aspect_results.append(aspect_sentiments)\n",
    "    \n",
    "    print(\"   üìã Aspect Sentiments:\")\n",
    "    for aspect, sentiment in aspect_sentiments.items():\n",
    "        emoji = \"üëç\" if sentiment['label'] == 'POSITIVE' else \"üëé\" if sentiment['label'] == 'NEGATIVE' else \"üòê\"\n",
    "        print(f\"     {emoji} {aspect}: {sentiment['label']} (confidence: {sentiment['score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Create aspect sentiment heatmap data\n",
    "aspect_df_data = []\n",
    "for i, aspects in enumerate(all_aspect_results):\n",
    "    for aspect, sentiment in aspects.items():\n",
    "        score = sentiment['score'] if sentiment['label'] == 'POSITIVE' else -sentiment['score']\n",
    "        aspect_df_data.append({\n",
    "            'Review': f'Review {i+1}',\n",
    "            'Aspect': aspect,\n",
    "            'Sentiment_Score': score,\n",
    "            'Label': sentiment['label']\n",
    "        })\n",
    "\n",
    "aspect_df = pd.DataFrame(aspect_df_data)\n",
    "print(\"üìä Aspect Sentiment Summary:\")\n",
    "if not aspect_df.empty:\n",
    "    print(aspect_df.groupby(['Review', 'Label']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428692d5",
   "metadata": {},
   "source": [
    "## Interactive Visualizations\n",
    "\n",
    "Creating interactive dashboards to explore sentiment and semantic parsing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd13265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive interactive visualizations\n",
    "def create_sentiment_dashboard():\n",
    "    \"\"\"Create an interactive sentiment analysis dashboard\"\"\"\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    all_sentiments = []\n",
    "    all_emotions = []\n",
    "    all_confidences = []\n",
    "    text_labels = []\n",
    "    \n",
    "    for i, text in enumerate(complex_texts):\n",
    "        results = analyze_complex_sentiment(text)\n",
    "        all_sentiments.append(results['basic_sentiment']['label'])\n",
    "        all_emotions.append(results['emotion']['label'])\n",
    "        all_confidences.append(results['basic_sentiment']['score'])\n",
    "        text_labels.append(f\"Text {i+1}\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Sentiment Distribution', 'Emotion Distribution', \n",
    "                       'Confidence Scores', 'Sentiment vs Emotion'),\n",
    "        specs=[[{'type': 'pie'}, {'type': 'bar'}],\n",
    "               [{'type': 'scatter'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Sentiment Distribution (Pie Chart)\n",
    "    sentiment_counts = Counter(all_sentiments)\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=list(sentiment_counts.keys()), \n",
    "               values=list(sentiment_counts.values()),\n",
    "               name=\"Sentiment\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Emotion Distribution (Bar Chart)\n",
    "    emotion_counts = Counter(all_emotions)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(emotion_counts.keys()), \n",
    "               y=list(emotion_counts.values()),\n",
    "               name=\"Emotions\",\n",
    "               marker_color='lightblue'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Confidence Scores (Scatter Plot)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=text_labels, \n",
    "                   y=all_confidences,\n",
    "                   mode='markers+lines',\n",
    "                   name=\"Confidence\",\n",
    "                   marker=dict(size=10, color='orange')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Sentiment vs Emotion Heatmap-style\n",
    "    sentiment_emotion_df = pd.DataFrame({\n",
    "        'Sentiment': all_sentiments,\n",
    "        'Emotion': all_emotions\n",
    "    })\n",
    "    cross_tab = pd.crosstab(sentiment_emotion_df['Sentiment'], sentiment_emotion_df['Emotion'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=cross_tab.columns,\n",
    "               y=cross_tab.sum(),\n",
    "               name=\"Sentiment-Emotion Cross\",\n",
    "               marker_color='lightgreen'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"üé≠ Comprehensive Sentiment Analysis Dashboard\",\n",
    "        title_x=0.5,\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Create the dashboard\n",
    "create_sentiment_dashboard()\n",
    "\n",
    "# Additional static visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Sentiment Timeline\n",
    "sentiments_for_plot = []\n",
    "for text in complex_texts:\n",
    "    result = basic_sentiment(text)[0]\n",
    "    score = result['score'] if result['label'] == 'POSITIVE' else -result['score']\n",
    "    sentiments_for_plot.append(score)\n",
    "\n",
    "ax1.plot(range(1, len(sentiments_for_plot) + 1), sentiments_for_plot, \n",
    "         marker='o', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax1.set_title('üìà Sentiment Score Timeline', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Text Number')\n",
    "ax1.set_ylabel('Sentiment Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Emotion Intensity Heatmap\n",
    "emotion_data = []\n",
    "for text in complex_texts:\n",
    "    result = emotion_classifier(text)[0]\n",
    "    emotion_data.append(result['score'])\n",
    "\n",
    "ax2.bar(range(1, len(emotion_data) + 1), emotion_data, color='skyblue', alpha=0.7)\n",
    "ax2.set_title('üòä Emotion Intensity Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Text Number')\n",
    "ax2.set_ylabel('Emotion Confidence')\n",
    "\n",
    "# 3. Confidence Distribution\n",
    "confidence_data = []\n",
    "for text in complex_texts:\n",
    "    result = basic_sentiment(text)[0]\n",
    "    confidence_data.append(result['score'])\n",
    "\n",
    "ax3.hist(confidence_data, bins=5, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "ax3.set_title('üìä Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Confidence Score')\n",
    "ax3.set_ylabel('Frequency')\n",
    "\n",
    "# 4. Aspect Sentiment Radar (if aspect data exists)\n",
    "if not aspect_df.empty:\n",
    "    # Create a simple aspect summary\n",
    "    aspects = aspect_df['Aspect'].unique()[:5]  # Top 5 aspects\n",
    "    aspect_scores = []\n",
    "    for aspect in aspects:\n",
    "        avg_score = aspect_df[aspect_df['Aspect'] == aspect]['Sentiment_Score'].mean()\n",
    "        aspect_scores.append(avg_score)\n",
    "    \n",
    "    ax4.bar(range(len(aspects)), aspect_scores, color='lightgreen', alpha=0.7)\n",
    "    ax4.set_title('üéØ Average Aspect Sentiments', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(range(len(aspects)))\n",
    "    ax4.set_xticklabels(aspects, rotation=45, ha='right')\n",
    "    ax4.set_ylabel('Average Sentiment Score')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No Aspect Data Available', \n",
    "             transform=ax4.transAxes, ha='center', va='center',\n",
    "             fontsize=12, style='italic')\n",
    "    ax4.set_title('üéØ Aspect Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Visualization Summary:\")\n",
    "print(f\"‚Ä¢ Analyzed {len(complex_texts)} complex texts\")\n",
    "print(f\"‚Ä¢ Average sentiment confidence: {np.mean([basic_sentiment(text)[0]['score'] for text in complex_texts]):.4f}\")\n",
    "print(f\"‚Ä¢ Most common emotion: {Counter([emotion_classifier(text)[0]['label'] for text in complex_texts]).most_common(1)[0][0]}\")\n",
    "print(f\"‚Ä¢ Sentiment distribution: {dict(Counter([basic_sentiment(text)[0]['label'] for text in complex_texts]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7926af4",
   "metadata": {},
   "source": [
    "## Comprehensive NLU Pipeline\n",
    "\n",
    "Combining semantic parsing and advanced sentiment analysis into a unified pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_nlu_pipeline(text, task_type=\"auto\"):\n",
    "    \"\"\"\n",
    "    Complete NLU pipeline combining semantic parsing and sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        task_type (str): Type of analysis - \"sentiment\", \"semantic\", \"auto\"\n",
    "    \"\"\"\n",
    "    print(\"üîç Comprehensive NLU Pipeline Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìù Input: {text}\")\n",
    "    print(f\"üéØ Task Type: {task_type}\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Determine task type automatically if needed\n",
    "    if task_type == \"auto\":\n",
    "        # Simple heuristics to determine primary task\n",
    "        if any(word in text.lower() for word in ['sql', 'query', 'database', 'table', 'select']):\n",
    "            task_type = \"semantic\"\n",
    "        elif any(word in text.lower() for word in ['feel', 'think', 'love', 'hate', 'good', 'bad']):\n",
    "            task_type = \"sentiment\"\n",
    "        else:\n",
    "            task_type = \"both\"\n",
    "    \n",
    "    # Semantic Parsing\n",
    "    if task_type in [\"semantic\", \"both\"]:\n",
    "        print(\"üóÉÔ∏è  Semantic Parsing Results:\")\n",
    "        \n",
    "        # Try SQL generation\n",
    "        if any(word in text.lower() for word in ['show', 'find', 'get', 'list', 'count']):\n",
    "            sql_result = generate_sql(text)\n",
    "            results['sql'] = sql_result\n",
    "            print(f\"   üíæ SQL: {sql_result}\")\n",
    "        \n",
    "        # Try structured extraction\n",
    "        structured_result = extract_structured_data(text)\n",
    "        results['structured_data'] = structured_result\n",
    "        print(f\"   üìä Structured: {structured_result}\")\n",
    "        print()\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    if task_type in [\"sentiment\", \"both\"]:\n",
    "        print(\"üé≠ Sentiment Analysis Results:\")\n",
    "        \n",
    "        # Multi-dimensional sentiment\n",
    "        sentiment_results = analyze_complex_sentiment(text)\n",
    "        results['sentiment'] = sentiment_results\n",
    "        \n",
    "        print(f\"   üòä Basic: {sentiment_results['basic_sentiment']['label']} ({sentiment_results['basic_sentiment']['score']:.4f})\")\n",
    "        print(f\"   üí´ Emotion: {sentiment_results['emotion']['label']} ({sentiment_results['emotion']['score']:.4f})\")\n",
    "        print(f\"   üå°Ô∏è  Intensity: {sentiment_results['intensity']['label']} ({sentiment_results['intensity']['score']:.4f})\")\n",
    "        \n",
    "        # Aspect-based sentiment if applicable\n",
    "        if len(text.split()) > 10:  # Only for longer texts\n",
    "            aspect_results = extract_aspects_and_sentiment(text)\n",
    "            results['aspects'] = aspect_results\n",
    "            print(\"   üéØ Aspects:\")\n",
    "            for aspect, sentiment in aspect_results.items():\n",
    "                emoji = \"üëç\" if sentiment['label'] == 'POSITIVE' else \"üëé\" if sentiment['label'] == 'NEGATIVE' else \"üòê\"\n",
    "                print(f\"     {emoji} {aspect}: {sentiment['label']} ({sentiment['score']:.4f})\")\n",
    "        print()\n",
    "    \n",
    "    # Text Statistics\n",
    "    words = text.split()\n",
    "    sentences = len([s for s in text.split('.') if s.strip()])\n",
    "    print(\"üìä Text Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Words: {len(words)}\")\n",
    "    print(f\"   ‚Ä¢ Characters: {len(text)}\")\n",
    "    print(f\"   ‚Ä¢ Sentences: {sentences}\")\n",
    "    print(f\"   ‚Ä¢ Avg words per sentence: {len(words)/max(sentences, 1):.1f}\")\n",
    "    \n",
    "    results['stats'] = {\n",
    "        'words': len(words),\n",
    "        'characters': len(text),\n",
    "        'sentences': sentences\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the comprehensive pipeline\n",
    "test_cases = [\n",
    "    # Semantic parsing examples\n",
    "    \"Show me all customers who made purchases greater than $500 in the last month\",\n",
    "    \"Extract the key information: John Doe, 30 years old, Software Engineer at Google, lives in California\",\n",
    "    \n",
    "    # Sentiment analysis examples\n",
    "    \"I absolutely love this new smartphone! The camera is amazing but the battery life could be better.\",\n",
    "    \"The hotel stay was disappointing - dirty rooms and terrible service, though the location was convenient.\",\n",
    "    \n",
    "    # Mixed examples\n",
    "    \"Find all employees with positive performance reviews and high customer satisfaction ratings\",\n",
    "    \"I'm excited about the new job opportunity at Microsoft, but worried about relocating to Seattle.\"\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"üß™ Test Case {i}\")\n",
    "    result = comprehensive_nlu_pipeline(test_case, task_type=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e305127",
   "metadata": {},
   "source": [
    "## Interactive Experimentation\n",
    "\n",
    "Try your own examples with the comprehensive NLU pipeline! Modify the variables below to test different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Interactive Experimentation Area\n",
    "# Modify these variables to test your own examples!\n",
    "\n",
    "# Your custom text input\n",
    "user_input = \"I need to find all customers from New York who gave us 5-star ratings, but I'm concerned about the recent complaints regarding delivery times.\"\n",
    "\n",
    "# Task type: \"semantic\", \"sentiment\", \"both\", or \"auto\"\n",
    "analysis_type = \"auto\"\n",
    "\n",
    "print(\"üß™ Your Custom NLU Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run the comprehensive analysis\n",
    "custom_result = comprehensive_nlu_pipeline(user_input, task_type=analysis_type)\n",
    "\n",
    "# Performance metrics\n",
    "print(\"‚ö° Performance Insights:\")\n",
    "if 'sentiment' in custom_result:\n",
    "    sentiment_confidence = custom_result['sentiment']['basic_sentiment']['score']\n",
    "    print(f\"   ‚Ä¢ Sentiment confidence: {sentiment_confidence:.4f}\")\n",
    "    \n",
    "    if sentiment_confidence > 0.8:\n",
    "        print(\"   ‚Ä¢ High confidence sentiment prediction ‚úÖ\")\n",
    "    elif sentiment_confidence > 0.6:\n",
    "        print(\"   ‚Ä¢ Moderate confidence sentiment prediction ‚ö†Ô∏è\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Low confidence sentiment prediction ‚ùå\")\n",
    "\n",
    "if 'stats' in custom_result:\n",
    "    complexity_score = custom_result['stats']['words'] / max(custom_result['stats']['sentences'], 1)\n",
    "    print(f\"   ‚Ä¢ Text complexity (words/sentence): {complexity_score:.1f}\")\n",
    "    \n",
    "    if complexity_score > 20:\n",
    "        print(\"   ‚Ä¢ High complexity text üî•\")\n",
    "    elif complexity_score > 10:\n",
    "        print(\"   ‚Ä¢ Medium complexity text üìä\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Simple text structure üìù\")\n",
    "\n",
    "# Suggestions for improvement\n",
    "print(\"\\nüí° Experiment Suggestions:\")\n",
    "print(\"   ‚Ä¢ Try different task types: 'semantic', 'sentiment', 'both'\")\n",
    "print(\"   ‚Ä¢ Test with longer texts for aspect-based sentiment analysis\")\n",
    "print(\"   ‚Ä¢ Use domain-specific language (SQL, reviews, technical docs)\")\n",
    "print(\"   ‚Ä¢ Mix positive and negative sentiments in the same text\")\n",
    "print(\"   ‚Ä¢ Include structured data like names, dates, and numbers\")\n",
    "\n",
    "# Quick test examples you can copy-paste\n",
    "print(\"\\nüìã Quick Test Examples (copy and paste into user_input):\")\n",
    "quick_examples = [\n",
    "    \"Generate SQL to show top 10 best-selling products with customer reviews above 4 stars\",\n",
    "    \"The new restaurant has amazing pasta and great atmosphere, but the service is incredibly slow and prices are too high\",\n",
    "    \"Extract contact info: Dr. Sarah Johnson, cardiologist at Mayo Clinic, phone: 555-123-4567, email: s.johnson@mayo.edu\",\n",
    "    \"I'm thrilled about the promotion but nervous about the increased responsibilities and longer hours\",\n",
    "    \"Find all employees hired after 2020 with performance ratings excellent or outstanding\"\n",
    "]\n",
    "\n",
    "for i, example in enumerate(quick_examples, 1):\n",
    "    print(f\"   {i}. {example}\")\n",
    "\n",
    "print(f\"\\nüéØ Current Analysis Summary:\")\n",
    "print(f\"   ‚Ä¢ Input length: {len(user_input)} characters\")\n",
    "print(f\"   ‚Ä¢ Analysis type: {analysis_type}\")\n",
    "print(f\"   ‚Ä¢ Components analyzed: {', '.join(custom_result.keys())}\")\n",
    "\n",
    "# Create a simple visualization for the current result\n",
    "if 'sentiment' in custom_result:\n",
    "    sentiment_data = custom_result['sentiment']\n",
    "    \n",
    "    # Simple bar chart of sentiment components\n",
    "    components = ['Basic Sentiment', 'Emotion', 'Intensity']\n",
    "    scores = [\n",
    "        sentiment_data['basic_sentiment']['score'],\n",
    "        sentiment_data['emotion']['score'],\n",
    "        sentiment_data['intensity']['score']\n",
    "    ]\n",
    "    labels = [\n",
    "        sentiment_data['basic_sentiment']['label'],\n",
    "        sentiment_data['emotion']['label'],\n",
    "        sentiment_data['intensity']['label']\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    bars = ax.bar(components, scores, color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
    "    \n",
    "    # Add labels on bars\n",
    "    for i, (bar, label) in enumerate(zip(bars, labels)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{label}\\n({height:.3f})', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_title('üé≠ Sentiment Analysis Components for Your Input', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Confidence Score')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚ú® Ready for your next experiment! Modify 'user_input' and 'analysis_type' above and re-run this cell.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
